import os
import re
import fitz
import json
import logging
import asyncio
import numpy as np
import torch
import aiofiles
import pickle
import random
import hashlib
from datetime import datetime
from collections import defaultdict
from typing import List, Dict, Tuple, Optional
from difflib import get_close_matches

from aiogram import Bot, Dispatcher, types
from aiogram.filters import CommandStart, Command
from aiogram.types import FSInputFile, InlineKeyboardButton, InlineKeyboardMarkup
from aiogram.enums import ParseMode
from aiogram.utils.keyboard import InlineKeyboardBuilder

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from transformers import pipeline, AutoModelForQuestionAnswering, AutoTokenizer

from natasha import Segmenter, Doc, NewsEmbedding, NewsMorphTagger, MorphVocab

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
class Config:
    CACHE_DIR = "cache"
    PDF_FILE_PATH = "/opt/bot/btr_260125.pdf"
    LOG_FILE_PATH = "zapros.txt"
    FEEDBACK_FILE = "feedback.pkl"
    MAX_MESSAGE_SIZE = 4000
    BOT_TOKEN = "7532994631:AAEHa4jEpi9U8JoISi9BlNfBmn8g5ETaXtQ"
    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    QA_MODEL_NAME = "timpal0l/mdeberta-v3-base-squad2"
    CLASSIFIER_MODEL = "cointegrated/rubert-tiny-toxicity"
    SIMILARITY_THRESHOLD = 0.25
    MAX_PAGES_TO_SHOW = 3
    LINKS_PER_PAGE_LIMIT = 5
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    FEEDBACK_WEIGHT = 0.3
    RETRAIN_INTERVAL = 86400  # 24 —á–∞—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
os.makedirs(Config.CACHE_DIR, exist_ok=True)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLP –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
segmenter = Segmenter()
embedding = NewsEmbedding()
morph_tagger = NewsMorphTagger(embedding)
morph_vocab = MorphVocab()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI –º–æ–¥–µ–ª–µ–π
logging.info(f"Device set to use {Config.DEVICE}")
embedder = SentenceTransformer(Config.EMBEDDING_MODEL).to(Config.DEVICE)
qa_tokenizer = AutoTokenizer.from_pretrained(Config.QA_MODEL_NAME)
qa_model = AutoModelForQuestionAnswering.from_pretrained(Config.QA_MODEL_NAME).to(Config.DEVICE)
classifier = pipeline(
    "text-classification",
    model=Config.CLASSIFIER_MODEL,
    device=0 if Config.DEVICE == "cuda" else -1
)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
class AIState:
    inverse_index: Dict[str, set] = defaultdict(set)
    page_embeddings: np.ndarray = None
    text_cache: Dict[int, str] = {}
    unique_words: set = set()
    feedback: Dict[Tuple[str, int], List[Tuple[int, bool]]] = defaultdict(list)
    search_weights = {'keyword': 0.4, 'semantic': 0.4, 'feedback': 0.2}

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞
bot = Bot(token=Config.BOT_TOKEN)
dp = Dispatcher()

def escape_html(text: str) -> str:
    """–≠–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤ HTML"""
    if text is None:
        return ""
    return (
        str(text)
        .replace("&", "&amp;")
        .replace("<", "&lt;")
        .replace(">", "&gt;")
        .replace('"', "&quot;")
    )

async def initialize_data():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ"""
    await load_cached_data()
    if AIState.page_embeddings is None:
        await precompute_embeddings()
    if not AIState.inverse_index:
        await build_inverse_index()
    await load_feedback()
    logging.info("Data initialization completed")

async def load_cached_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    try:
        if os.path.exists("page_embeddings.npy"):
            AIState.page_embeddings = np.load("page_embeddings.npy")
        if os.path.exists("text_cache.pkl"):
            with open("text_cache.pkl", "rb") as f:
                AIState.text_cache = pickle.load(f)
        if os.path.exists("inverse_index.pkl"):
            with open("inverse_index.pkl", "rb") as f:
                AIState.inverse_index = pickle.load(f)
                AIState.unique_words = set(AIState.inverse_index.keys())
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {e}")

async def load_feedback():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏"""
    try:
        if os.path.exists(Config.FEEDBACK_FILE):
            with open(Config.FEEDBACK_FILE, "rb") as f:
                AIState.feedback = pickle.load(f)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏: {e}")

async def save_feedback():
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏"""
    try:
        with open(Config.FEEDBACK_FILE, "wb") as f:
            pickle.dump(AIState.feedback, f)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏: {e}")

async def precompute_embeddings():
    """–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü"""
    try:
        doc = fitz.open(Config.PDF_FILE_PATH)
        texts = [page.get_text().replace("\n", " ") for page in doc]
        AIState.text_cache = {i+1: text for i, text in enumerate(texts)}
        
        embeddings = await asyncio.to_thread(
            lambda: embedder.encode(texts, convert_to_tensor=True, device=Config.DEVICE)
        )
        AIState.page_embeddings = embeddings.cpu().numpy()
        np.save("page_embeddings.npy", AIState.page_embeddings)
        
        with open("text_cache.pkl", "wb") as f:
            pickle.dump(AIState.text_cache, f)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")

async def build_inverse_index():
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞"""
    try:
        doc = fitz.open(Config.PDF_FILE_PATH)
        for page_num in range(len(doc)):
            text = doc[page_num].get_text()
            lemmas = lemmatize_text(text)
            for lemma in lemmas:
                AIState.inverse_index[lemma].add(page_num + 1)
        
        AIState.unique_words = set(AIState.inverse_index.keys())
        with open("inverse_index.pkl", "wb") as f:
            pickle.dump(AIState.inverse_index, f)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: {e}")

def lemmatize_text(text: str) -> List[str]:
    """–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
    doc = Doc(text)
    doc.segment(segmenter)
    doc.tag_morph(morph_tagger)
    return [morph_vocab.lemmatize(token.text, token.pos, token.feats) for token in doc.tokens]

async def cache_page(page_number: int) -> Tuple[str, List[str]]:
    """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫"""
    try:
        doc = fitz.open(Config.PDF_FILE_PATH)
        page = doc.load_page(page_number - 1)
        
        img_path = os.path.join(Config.CACHE_DIR, f"page_{page_number}.png")
        if not os.path.exists(img_path):
            pix = page.get_pixmap()
            pix.save(img_path)
        
        links_path = os.path.join(Config.CACHE_DIR, f"links_{page_number}.json")
        links = []
        
        if os.path.exists(links_path):
            async with aiofiles.open(links_path, "r") as f:
                links = json.loads(await f.read())
        else:
            raw_links = page.get_links()
            links = [link.get("uri", "") for link in raw_links if link.get("uri")]
            async with aiofiles.open(links_path, "w") as f:
                await f.write(json.dumps(links, ensure_ascii=False))
        
        return img_path, links[:Config.LINKS_PER_PAGE_LIMIT]
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number}: {e}")
        return None, []

async def semantic_search(query: str) -> List[Tuple[int, float]]:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫"""
    try:
        query_embedding = embedder.encode([query], convert_to_tensor=True, device=Config.DEVICE)
        cos_scores = cosine_similarity(
            query_embedding.cpu(), 
            AIState.page_embeddings
        )[0]
        return sorted(enumerate(cos_scores, 1), key=lambda x: x[1], reverse=True)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
        return []

async def keyword_search(query: str) -> List[int]:
    """–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
    try:
        query_lemmas = lemmatize_text(query.lower())
        if not query_lemmas:
            return []
            
        pages = set()
        for lemma in query_lemmas:
            if lemma in AIState.inverse_index:
                if not pages:
                    pages = set(AIState.inverse_index[lemma])
                else:
                    pages &= AIState.inverse_index[lemma]
                if not pages:
                    break
        return sorted(pages)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ keyword –ø–æ–∏—Å–∫–∞: {e}")
        return []

async def hybrid_search(query: str) -> List[int]:
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ —Å —É—á–µ—Ç–æ–º –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏"""
    try:
        keyword_pages = await keyword_search(query)
        semantic_results = await semantic_search(query)
        semantic_pages = [p for p, s in semantic_results if s > Config.SIMILARITY_THRESHOLD]
        
        feedback_scores = defaultdict(float)
        for (q, p), fbs in AIState.feedback.items():
            if q.lower() == query.lower():
                positive = sum(1 for fb in fbs if fb[1])
                total = len(fbs)
                if total > 0:
                    feedback_scores[p] = positive / total
        
        combined = set(keyword_pages + semantic_pages)
        scores = {}
        
        for page in combined:
            keyword_score = 1 if page in keyword_pages else 0
            semantic_score = next((s for p, s in semantic_results if p == page), 0)
            feedback_score = feedback_scores.get(page, 0)
            
            scores[page] = (
                AIState.search_weights['keyword'] * keyword_score +
                AIState.search_weights['semantic'] * semantic_score +
                AIState.search_weights['feedback'] * feedback_score
            )
        
        return sorted(scores.keys(), key=lambda x: scores[x], reverse=True)[:Config.MAX_PAGES_TO_SHOW]
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
        return []

async def generate_answer(query: str, context: str) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞"""
    try:
        inputs = qa_tokenizer(
            query, 
            context, 
            max_length=512,
            truncation=True,
            return_tensors="pt"
        ).to(Config.DEVICE)
        
        with torch.no_grad():
            outputs = qa_model(**inputs)
            
        answer_start = torch.argmax(outputs.start_logits)
        answer_end = torch.argmax(outputs.end_logits) + 1
        
        answer = qa_tokenizer.decode(
            inputs["input_ids"][0][answer_start:answer_end],
            skip_special_tokens=True
        )
        
        return answer if answer else "–û—Ç–≤–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ"
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
        return "–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç"

async def log_query(user: types.User, query: str, results: List[int]):
    """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤"""
    try:
        safe_username = escape_html(user.username or "")
        safe_fullname = escape_html(user.full_name or "")
        safe_query = escape_html(query or "")
        
        log_entry = (
            f"{datetime.now():%Y-%m-%d %H:%M:%S}; "
            f"{user.id}; "
            f"{safe_username}; "
            f"{safe_fullname}; "
            f"{safe_query}; "
            f"{', '.join(map(str, results))}\n"
        )
        
        async with aiofiles.open(Config.LOG_FILE_PATH, "a") as f:
            await f.write(log_entry)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")

#–∑–∞–∫–æ–º–µ–Ω—Ç–∏–ª —Ñ—É–Ω–∫—Ü–∏—é create_feedback_keyboard –¥–ª—è –æ—Ç–∫–ª—é—á–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è –∑–∞ –ø–æ—Å—Ç—ã
""" 
def create_feedback_keyboard(pages: List[int]) -> InlineKeyboardMarkup:
#    –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã —Å –∫–Ω–æ–ø–∫–∞–º–∏ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏
    builder = InlineKeyboardBuilder()
    for page in pages:
        builder.row(
            InlineKeyboardButton(
                text=f"üëç –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}",
                callback_data=f"fb_{page}_1"
            ),
            InlineKeyboardButton(
                text=f"üëé –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}",
                callback_data=f"fb_{page}_0"
            )
        )
    return builder.as_markup()
"""

@dp.message(CommandStart())
async def start_command(message: types.Message):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã /start"""
    await message.answer(
        "üîç –î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏!\n\n"
        "–û—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å, –∏ —è –Ω–∞–π–¥—É —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é."
    )

@dp.message(Command("help"))
async def help_command(message: types.Message):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã /help"""
    help_text = (
        "üõ† **–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:**\n\n"
        "/start - –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –±–æ—Ç–∞\n"
        "/help - –°–ø—Ä–∞–≤–∫–∞\n"
        "/history - –ò—Å—Ç–æ—Ä–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤\n\n"
        "üîé **–ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:**\n"
        "‚Ä¢ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞\n"
        "‚Ä¢ –†–µ—à–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ 404\n"
        "‚Ä¢ –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"
    )
    await message.answer(help_text, parse_mode=ParseMode.HTML)

@dp.message(Command("history"))
async def history_command(message: types.Message):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã /history"""
    try:
        if not os.path.exists(Config.LOG_FILE_PATH):
            await message.answer("–ò—Å—Ç–æ—Ä–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –ø—É—Å—Ç–∞.")
            return
            
        async with aiofiles.open(Config.LOG_FILE_PATH, "r") as f:
            content = await f.read()
            
        user_entries = [
            line for line in content.splitlines() 
            if str(message.from_user.id) in line
        ]
        
        if not user_entries:
            await message.answer("–í–∞—à–∞ –∏—Å—Ç–æ—Ä–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –ø—É—Å—Ç–∞.")
            return
            
        response = "üìú **–ò—Å—Ç–æ—Ä–∏—è –≤–∞—à–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤:**\n\n"
        for entry in user_entries[-5:]:
            parts = entry.split("; ")
            response += (
                f"üìÖ *{parts[0]}*\n"
                f"üîé *–ó–∞–ø—Ä–æ—Å:* {parts[4]}\n"
                f"üìë *–°—Ç—Ä–∞–Ω–∏—Ü—ã:* {parts[5]}\n\n"
            )
            
        await message.answer(response, parse_mode=ParseMode.HTML)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏: {e}")
        await message.answer("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é.")

""" –æ—Ç–∫–ª—é—á–∏–ª –¥–ª—è –æ—Ç–∫–ª—é—á–µ–Ω–∏—è –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è
@dp.callback_query(lambda c: c.data.startswith('fb_'))
async def handle_feedback(callback: types.CallbackQuery):
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏
    try:
        _, page, feedback = callback.data.split('_')
        query = callback.message.text.split("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É:")[1].split("\n")[0].strip().strip('`')
        
        AIState.feedback[(query, int(page))].append(
            (callback.from_user.id, bool(int(feedback)))
	)
        
        await save_feedback()
        await adjust_search_weights()
        await callback.answer("–°–ø–∞—Å–∏–±–æ –∑–∞ –≤–∞—à –æ—Ç–∑—ã–≤! –≠—Ç–æ –ø–æ–º–æ–∂–µ—Ç —É–ª—É—á—à–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞.")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏: {e}")
        await callback.answer("‚ö†Ô∏è –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –æ—Ç–∑—ã–≤–∞.")
"""

async def adjust_search_weights():
    """–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –≤–µ—Å–æ–≤ –ø–æ–∏—Å–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏"""
    try:
        total_feedback = sum(len(fbs) for fbs in AIState.feedback.values())
        if total_feedback == 0:
            return

        positive = sum(1 for fbs in AIState.feedback.values() for fb in fbs if fb[1])
        positive_ratio = positive / total_feedback
        
        AIState.search_weights['feedback'] = min(0.5, positive_ratio)
        AIState.search_weights['keyword'] = 0.4 - (positive_ratio * 0.2)
        AIState.search_weights['semantic'] = 0.4 + (positive_ratio * 0.2)
        
        total = sum(AIState.search_weights.values())
        for k in AIState.search_weights:
            AIState.search_weights[k] /= total
        
        logging.info(f"–û–±–Ω–æ–≤–ª–µ–Ω—ã –≤–µ—Å–∞ –ø–æ–∏—Å–∫–∞: {AIState.search_weights}")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∫–∏ –≤–µ—Å–æ–≤: {e}")

def pdf_has_changed():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ PDF-—Ñ–∞–π–ª–µ"""
    try:
        current_hash = hashlib.md5(open(Config.PDF_FILE_PATH, "rb").read()).hexdigest()
        saved_hash = ""
        if os.path.exists("pdf_hash.txt"):
            with open("pdf_hash.txt", "r") as f:
                saved_hash = f.read()
        
        if current_hash != saved_hash:
            with open("pdf_hash.txt", "w") as f:
                f.write(current_hash)
            return True
        return False
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π PDF: {e}")
        return False

async def periodic_retraining():
    """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
    while True:
        await asyncio.sleep(Config.RETRAIN_INTERVAL)
        try:
            logging.info("–ù–∞—á–∞–ª–æ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è")
            
            if pdf_has_changed():
                logging.info("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ PDF")
                await precompute_embeddings()
                await build_inverse_index()
            
            await adjust_search_weights()
            logging.info("–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
            
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è: {e}")

@dp.message()
async def handle_query(message: types.Message):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
    try:
        user = message.from_user
        query = message.text.strip()
        
        if len(query) < 3:
            await message.answer("‚ùå –ó–∞–ø—Ä–æ—Å –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–∏–Ω–∏–º—É–º 3 —Å–∏–º–≤–æ–ª–∞.")
            return
            
        toxicity = await asyncio.to_thread(classifier, query)
        if toxicity[0]['label'] == 'toxic':
            await message.answer("üö´ –ó–∞–ø—Ä–æ—Å —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è.")
            return
            
        found_pages = await hybrid_search(query)
        await log_query(user, query, found_pages)
        
        if not found_pages:
            suggestions = get_close_matches(query, AIState.unique_words, n=5, cutoff=0.4)
            response = "üîç –ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."
            if suggestions:
                response += "\n\n–í–æ–∑–º–æ–∂–Ω–æ, –≤—ã –∏–º–µ–ª–∏ –≤ –≤–∏–¥—É:\n" + "\n".join(f"‚Ä¢ {s}" for s in suggestions)
            await message.answer(response, parse_mode=ParseMode.HTML)
            return
            
        context = "\n".join(AIState.text_cache[p] for p in found_pages)
        answer = await generate_answer(query, context)
        
        response = (
            f"üîç <b>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É:</b> <code>{escape_html(query)}</code>\n\n"
            f"{escape_html(answer)}\n\n"
            f"üìñ <b>–°—Ç—Ä–∞–Ω–∏—Ü—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏:</b> {', '.join(map(str, found_pages))}"
        )


#        keyboard = create_feedback_keyboard(found_pages) #—É–±—Ä–∞–ª –∫–ª–∞–≤–∏–∞—Ç—É—Ä—É –æ–±—É—á–µ–Ω–∏—è
#        await message.answer(response, parse_mode=ParseMode.HTML, reply_markup=keyboard) #—É–±—Ä–∞–ª –∫–ª–∞–≤–∏–∞—Ç—É—Ä—É –æ–±—É—á–µ–Ω–∏—è
        await message.answer(response, parse_mode=ParseMode.HTML) #—É–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫—É –ø–æ—Å–ª–µ –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏—è –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã –æ–±—É—á–µ–Ω–∏—è

        for page_num in found_pages[:Config.MAX_PAGES_TO_SHOW]:
            img_path, links = await cache_page(page_num)
            
            if img_path and os.path.exists(img_path):
                await message.answer_photo(FSInputFile(img_path))
            
            if links:
                formatted_links = []
                for i, link in enumerate(links, 1):
                    if link and isinstance(link, str):
                        safe_url = escape_html(link.strip())
                        formatted_links.append(
                            f"{i}. <a href='{safe_url}'>–ü–µ—Ä–µ–π—Ç–∏ –ø–æ —Å—Å—ã–ª–∫–µ</a>"
                        )
                
                if formatted_links:
                    links_text = "\n".join(formatted_links)
                    await message.answer(
                        f"üîó <b>–°—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}:</b>\n{links_text}",
                        parse_mode=ParseMode.HTML,
                        disable_web_page_preview=True
                    )
                
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
        await message.answer("‚ö†Ô∏è –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞.")

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    await initialize_data()
    asyncio.create_task(periodic_retraining())
    await dp.start_polling(bot)

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    asyncio.run(main())
