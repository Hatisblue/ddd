# —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
# –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü –∏ —Å—Å—ã–ª–æ–∫
# –≤—ã–≤–æ–¥ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è –ø–æ–¥ –∫–∞–∂–¥—ã–º —Å–ª–∞–π–¥–æ–º
# –≤—ã–≤–æ–¥ —Å—Å—ã–ª–æ–∫ –ø–æ–¥ –∫–∞–∂–¥—ã–º —Å–ª–∞–π–¥–æ–º
# –æ–±—Ä–µ–∑–∞–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è —Å—Å—ã–ª–æ–∫ –¥–æ 30 –∑–Ω–∞–∫–æ–≤
# —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—é –ø–æ –∫–æ–º–∞–Ω–¥–µ _Stats999_

# https://docs.google.com/presentation/d/1zN4KBxC-61DcMk1uG9RTOaOGcKC9UGKWC3wEDRprtF8/edit?slide=id.g2e819df673a_0_0#slide=id.g2e819df673a_0_0
# –ë–¢–† - –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–ª–∞–π–¥–æ–≤ –≤ –∫–æ–Ω–µ—Ü –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏, –ø–æ—Ä—è–¥–æ–∫ —Å–ª–∞–π–¥–æ–≤ 1-127 –Ω–µ –º–µ–Ω—è—Ç—å –∏ —Å–ª–∞–π–¥—ã –Ω–µ —É–¥–∞–ª—è—Ç—å

# - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è –≤ –º–æ–¥–µ–ª—å
# - —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è —Å–±–æ—Ä–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —Å–ª–∞–π–¥–∞–º
# - –¥–æ–±–∞–≤–∏—Ç—å –∫–æ–º–∞–Ω–¥—É –¥–ª—è –≤—ã–¥–∞—á–∏ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Å–ª–∞–π–¥–æ–≤

import os
import re
import fitz
import json
from config_bot import TOKEN
import torch
import pickle
import random
import logging
import asyncio
import aiofiles
import hashlib
import numpy as np
from datetime import datetime
import collections
from collections import defaultdict
from typing import List, Dict, Tuple, Optional
from difflib import get_close_matches
from aiogram import Bot, Dispatcher, types
from aiogram.filters import CommandStart, Command
from aiogram.types import FSInputFile, InlineKeyboardButton, InlineKeyboardMarkup
from aiogram.enums import ParseMode
from aiogram.utils.keyboard import InlineKeyboardBuilder
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from transformers import pipeline, AutoModelForQuestionAnswering, AutoTokenizer
from natasha import Segmenter, Doc, NewsEmbedding, NewsMorphTagger, MorphVocab



# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
class Config:

    BOT_TOKEN = TOKEN

    CACHE_DIR = "cache"
    PDF_FILE_PATH = "btr3006.pdf"
    LOG_FILE_PATH = "zapros.txt"
    FEEDBACK_FILE = "feedback.pkl"
    MAX_MESSAGE_SIZE = 4000
    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    QA_MODEL_NAME = "timpal0l/mdeberta-v3-base-squad2"
    CLASSIFIER_MODEL = "cointegrated/rubert-tiny-toxicity"
    SIMILARITY_THRESHOLD = 0.28
    MAX_PAGES_TO_SHOW = 5 # –õ–∏–º–∏—Ç –≤—ã–¥–∞—á–∏ —Å–ª–∞–π–¥–æ–≤
    MIN_RELEVANCE_SCORE = 0.52  # –ü–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    CLOSE_MATCH_CUTOFF = 0.72  # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    LINKS_PER_PAGE_LIMIT = 5
    DEVICE = "cpu"
    # "cuda" if torch.cuda.is_available() else "cpu"
    FEEDBACK_WEIGHT = 0.3
    RETRAIN_INTERVAL = 86400  # 24 —á–∞—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
os.makedirs(Config.CACHE_DIR, exist_ok=True)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLP –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
segmenter = Segmenter()
embedding = NewsEmbedding()
morph_tagger = NewsMorphTagger(embedding)
morph_vocab = MorphVocab()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI –º–æ–¥–µ–ª–µ–π
logging.info(f"Device set to use {Config.DEVICE}")
embedder = SentenceTransformer(Config.EMBEDDING_MODEL).to(Config.DEVICE)
qa_tokenizer = AutoTokenizer.from_pretrained(Config.QA_MODEL_NAME)
qa_model = AutoModelForQuestionAnswering.from_pretrained(Config.QA_MODEL_NAME).to(Config.DEVICE)
classifier = pipeline(
    "text-classification",
    model=Config.CLASSIFIER_MODEL,
    device=-1  # if Config.DEVICE == "cuda" else -1
)


# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
class AIState:
    inverse_index: Dict[str, set] = defaultdict(set)
    page_embeddings: np.ndarray = None
    text_cache: Dict[int, str] = {}
    unique_words: set = set()
    feedback: Dict[Tuple[str, int], List[Tuple[int, bool]]] = defaultdict(list)
    search_weights = {'keyword': 0.4, 'semantic': 0.4, 'feedback': 0.2}


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞
bot = Bot(token=Config.BOT_TOKEN)
dp = Dispatcher()

# –º–æ–¥—É–ª—å –≤—ã–≤–æ–¥–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—é –ø–æ –∫–æ–º–∞–Ω–¥–µ Stats999
PKL_FILE_PATH = 'feedback.pkl' # –∏–∑–º–µ–Ω–∏—Ç—å –ø—É—Ç—å –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ


def load_feedback_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PKL —Ñ–∞–π–ª–∞"""
    try:
        with open(PKL_FILE_PATH, 'rb') as f:
            data = pickle.load(f, encoding='latin1')
    except UnicodeDecodeError:
        with open(PKL_FILE_PATH, 'rb') as f:
            data = pickle.load(f, encoding='bytes')

    def convert(item):
        if isinstance(item, bytes):
            return item.decode('latin1', errors='replace')
        elif isinstance(item, list):
            return [convert(i) for i in item]
        elif isinstance(item, tuple):
            return tuple(convert(i) for i in item)
        elif isinstance(item, dict):
            return {convert(k): convert(v) for k, v in item.items()}
        elif isinstance(item, collections.defaultdict):
            return {convert(k): convert(v) for k, v in item.items()}
        return item

    return convert(data)


def aggregate_feedback_stats(decoded_data):
    """–ê–≥—Ä–µ–≥–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –≥–æ–ª–æ—Å–∞–º"""
    aggregation = defaultdict(lambda: [0, 0])  # [true_count, false_count]

    for key, votes in decoded_data.items():
        number = key[1]
        for vote_tuple in votes:
            if vote_tuple[1]:  # True vote
                aggregation[number][0] += 1
            else:  # False vote
                aggregation[number][1] += 1

    return dict(sorted(aggregation.items(), key=lambda x: x[0]))


def format_stats_message(aggregated_data):
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏—è"""
    # –û—Å–Ω–æ–≤–Ω–æ–π —Å—Ç–∏–ª—å —Ç–µ–∫—Å—Ç–∞


    if not aggregated_data:
        return "‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"

    message = "üìä <b>–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥–æ–ª–æ—Å–æ–≤:</b>\n\n"
    message += "<code>–°–ª–∞–π–¥   |  üëç –î–∞  |  üëé –ù–µ—Ç  | –í—Å–µ–≥–æ</code>\n"
    message += "<code>----------------------------------</code>\n"

    for number, (true_count, false_count) in aggregated_data.items():
        total = true_count + false_count
        message += f"<code>{number:^7} | {true_count:^6} | {false_count:^7} | {total:^5}</code>\n"

    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_true = sum(true for true, _ in aggregated_data.values())
    total_false = sum(false for _, false in aggregated_data.values())
    total_all = total_true + total_false

    message += "\n<b>–ò—Ç–æ–≥–æ:</b>\n"
    message += f"üëç –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö: {total_true}\n"
    message += f"üëé –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö: {total_false}\n"
    message += f"üìà –í—Å–µ–≥–æ –≥–æ–ª–æ—Å–æ–≤: {total_all}"

    return message

def escape_html(text: str) -> str:
    """–≠–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤ HTML"""
    if text is None:
        return ""
    return (
        str(text)
        .replace("&", "&amp;")
        .replace("<", "&lt;")
        .replace(">", "&gt;")
        .replace('"', "&quot;")
    )


async def initialize_data():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ"""
    await load_cached_data()
    if AIState.page_embeddings is None:
        await precompute_embeddings()
    if not AIState.inverse_index:
        await build_inverse_index()
    await load_feedback()
    logging.info("Data initialization completed")


async def load_cached_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    try:
        if os.path.exists("page_embeddings.npy"):
            AIState.page_embeddings = np.load("page_embeddings.npy")
        if os.path.exists("text_cache.pkl"):
            with open("text_cache.pkl", "rb") as f:
                AIState.text_cache = pickle.load(f)
        if os.path.exists("inverse_index.pkl"):
            with open("inverse_index.pkl", "rb") as f:
                AIState.inverse_index = pickle.load(f)
                AIState.unique_words = set(AIState.inverse_index.keys())
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {e}")


async def load_feedback():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏"""
    try:
        if os.path.exists(Config.FEEDBACK_FILE):
            with open(Config.FEEDBACK_FILE, "rb") as f:
                AIState.feedback = pickle.load(f)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏: {e}")


async def save_feedback():
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏"""
    try:
        with open(Config.FEEDBACK_FILE, "wb") as f:
            pickle.dump(AIState.feedback, f)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏: {e}")


async def precompute_embeddings():
    """–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü"""
    try:
        doc = fitz.open(Config.PDF_FILE_PATH)
        texts = [page.get_text().replace("\n", " ") for page in doc]
        AIState.text_cache = {i + 1: text for i, text in enumerate(texts)}

        embeddings = await asyncio.to_thread(
            lambda: embedder.encode(texts, convert_to_tensor=True, device=Config.DEVICE)
        )
        AIState.page_embeddings = embeddings.cpu().numpy()
        np.save("page_embeddings.npy", AIState.page_embeddings)

        with open("text_cache.pkl", "wb") as f:
            pickle.dump(AIState.text_cache, f)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")


async def build_inverse_index():
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞"""
    try:
        doc = fitz.open(Config.PDF_FILE_PATH)
        for page_num in range(len(doc)):
            text = doc[page_num].get_text()
            lemmas = lemmatize_text(text)
            for lemma in lemmas:
                AIState.inverse_index[lemma].add(page_num + 1)

        AIState.unique_words = set(AIState.inverse_index.keys())
        with open("inverse_index.pkl", "wb") as f:
            pickle.dump(AIState.inverse_index, f)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: {e}")


def lemmatize_text(text: str) -> List[str]:
    """–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
    doc = Doc(text)
    doc.segment(segmenter)
    doc.tag_morph(morph_tagger)
    return [morph_vocab.lemmatize(token.text, token.pos, token.feats) for token in doc.tokens]

#–æ–±—Ä–µ–∑–∞–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è —Å—Å—ã–ª–∫–∏
def truncate_text(
        text: str,
        max_length: int = 30, #–¥–ª–∏–Ω–Ω–∞ –æ–ø–∏—Å–∞–Ω–∏—è —Å—Å—ã–ª–∫–∏
        common_prefix: str = "—Å–∫–æ—Ç—á–ª–æ–∫–∞. –ü—Ä–∏–º–µ—Ä–Ω—ã–π –∫–æ–º–ø–ª–µ–∫—Ç –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:"
) -> str:
    """–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å—Å—ã–ª–∫–∏ –∏ —É–¥–∞–ª–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤"""
    if not text:
        return "–°—Å—ã–ª–∫–∞ –±–µ–∑ —Ç–µ–∫—Å—Ç–∞"

    # –£–¥–∞–ª–µ–Ω–∏–µ –æ–±—â–µ–≥–æ –ø—Ä–µ—Ñ–∏–∫—Å–∞ (—Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤)
    clean_text = text.strip()
    prefix = common_prefix.strip()

    if clean_text.lower().startswith(prefix.lower()):
        # –£–¥–∞–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –∏ –≤—Å–µ —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞ –Ω–∏–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
        clean_text = re.sub(f"^{re.escape(prefix)}[\\s,.;:]*", "", clean_text, flags=re.IGNORECASE)

    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
    if len(clean_text) <= max_length:
        return clean_text or "–°—Å—ã–ª–∫–∞"

    # –°–æ–∫—Ä–∞—â–∞–µ–º –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ü–µ–ª–æ–≥–æ —Å–ª–æ–≤–∞
    truncated = clean_text[:max_length]
    last_space = truncated.rfind(" ")

    if last_space > 0:
        result = truncated[:last_space] + "‚Ä¶"
    else:
        # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä–æ–±–µ–ª–æ–≤, –æ–±—Ä–µ–∑–∞–µ–º –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–∏–º–≤–æ–ª–∞ –∏ –¥–æ–±–∞–≤–ª—è–µ–º –º–Ω–æ–≥–æ—Ç–æ—á–∏–µ
        result = clean_text[:max_length - 1] + "‚Ä¶" if max_length > 1 else clean_text[0] + "‚Ä¶"

    return result


async def cache_page(page_number: int) -> Tuple[str, List[Tuple[str, str]]]:
    """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ —Å —Ç–µ–∫—Å—Ç–æ–º"""
    try:
        doc = fitz.open(Config.PDF_FILE_PATH)
        page = doc.load_page(page_number - 1)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        img_path = os.path.join(Config.CACHE_DIR, f"page_{page_number}.png")
        if not os.path.exists(img_path):
            pix = page.get_pixmap()
            pix.save(img_path)

        links_path = os.path.join(Config.CACHE_DIR, f"links_{page_number}.json")
        links = []

        if os.path.exists(links_path):
            async with aiofiles.open(links_path, "r") as f:
                cached_links = json.loads(await f.read())
                links = [(item["text"], item["uri"]) for item in cached_links]
        else:
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –±–ª–æ–∫–∏
            text_blocks = page.get_text("dict")["blocks"]
            raw_links = page.get_links()

            for link in raw_links:
                if not link.get("uri"):
                    continue
                rect = fitz.Rect(link["from"])
                link_text = ""

                # –ü–æ–∏—Å–∫ —Ç–µ–∫—Å—Ç–∞ –≤ –æ–±–ª–∞—Å—Ç–∏ —Å—Å—ã–ª–∫–∏
                for block in text_blocks:
                    if block["type"] == 0:
                        for line in block["lines"]:
                            for span in line["spans"]:
                                span_rect = fitz.Rect(span["bbox"])
                                if rect.intersects(span_rect):
                                    link_text += span["text"]

                # –ß–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
                link_text = re.sub(r"\s+", " ", link_text).strip()
                if not link_text:
                    link_text = "–°—Å—ã–ª–∫–∞"
                else:
                    link_text = truncate_text(link_text)

                links.append((link_text, link["uri"]))  # –°–æ—Ö—Ä–∞–Ω—è–µ–º (—Ç–µ–∫—Å—Ç, URI)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
            async with aiofiles.open(links_path, "w") as f:
                serialized_links = [{"text": text, "uri": uri} for text, uri in links]
                await f.write(json.dumps(serialized_links, ensure_ascii=False))

        return img_path, links[:Config.LINKS_PER_PAGE_LIMIT]
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ª–∞–π–¥–∞ {page_number}: {e}")
        return None, []


async def semantic_search(query: str) -> List[Tuple[int, float]]:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫"""
    try:
        query_embedding = embedder.encode([query], convert_to_tensor=True, device=Config.DEVICE)
        cos_scores = cosine_similarity(
            query_embedding.cpu(),
            AIState.page_embeddings
        )[0]
        return sorted(enumerate(cos_scores, 1), key=lambda x: x[1], reverse=True)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
        return []


async def keyword_search(query: str) -> List[int]:
    """–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
    try:
        query_lemmas = lemmatize_text(query.lower())
        if not query_lemmas:
            return []

        pages = set()
        for lemma in query_lemmas:
            if lemma in AIState.inverse_index:
                if not pages:
                    pages = set(AIState.inverse_index[lemma])
                else:
                    pages &= AIState.inverse_index[lemma]
                if not pages:
                    break
        return sorted(pages)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ keyword –ø–æ–∏—Å–∫–∞: {e}")
        return []


async def hybrid_search(query: str) -> List[int]:
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ —Å —É—á–µ—Ç–æ–º –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏"""
    try:
        keyword_pages = await keyword_search(query)
        semantic_results = await semantic_search(query)
        semantic_pages = [p for p, s in semantic_results if s > Config.SIMILARITY_THRESHOLD]

        feedback_scores = defaultdict(float)
        for (q, p), fbs in AIState.feedback.items():
            if q.lower() == query.lower():
                positive = sum(1 for fb in fbs if fb[1])
                total = len(fbs)
                if total > 0:
                    feedback_scores[p] = positive / total

        combined = set(keyword_pages + semantic_pages)
        scores = {}

        for page in combined:
            keyword_score = 1 if page in keyword_pages else 0
            semantic_score = next((s for p, s in semantic_results if p == page), 0)
            feedback_score = feedback_scores.get(page, 0)

            scores[page] = (
                    AIState.search_weights['keyword'] * keyword_score +
                    AIState.search_weights['semantic'] * semantic_score +
                    AIState.search_weights['feedback'] * feedback_score
            )

        return sorted(scores.keys(), key=lambda x: scores[x], reverse=True)[:Config.MAX_PAGES_TO_SHOW]
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
        return []


async def generate_answer(query: str, context: str) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞"""
    try:
        inputs = qa_tokenizer(
            query,
            context,
            max_length=512,
            truncation=True,
            return_tensors="pt"
        ).to(Config.DEVICE)

        with torch.no_grad():
            outputs = qa_model(**inputs)

        answer_start = torch.argmax(outputs.start_logits)
        answer_end = torch.argmax(outputs.end_logits) + 1

        answer = qa_tokenizer.decode(
            inputs["input_ids"][0][answer_start:answer_end],
            skip_special_tokens=True
        )

        return answer if answer else "–û—Ç–≤–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–¢–†"
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
        return "–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç"


async def log_query(user: types.User, query: str, results: List[int]):
    """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤"""
    try:
        safe_username = escape_html(user.username or "")
        safe_fullname = escape_html(user.full_name or "")
        safe_query = escape_html(query or "")

        log_entry = (
            f"{datetime.now():%Y-%m-%d %H:%M:%S}; "
            f"{user.id}; "
            f"{safe_username}; "
            f"{safe_fullname}; "
            f"{safe_query}; "
            f"{', '.join(map(str, results))}\n"
        )

        async with aiofiles.open(Config.LOG_FILE_PATH, "a") as f:
            await f.write(log_entry)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")


def create_feedback_keyboard(query: str, pages: List[int]) -> InlineKeyboardMarkup:
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã —Å –∫–Ω–æ–ø–∫–∞–º–∏ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –∏ —Ö—ç—à–µ–º –∑–∞–ø—Ä–æ—Å–∞"""
    builder = InlineKeyboardBuilder()
    query_hash = hashlib.md5(query.encode()).hexdigest()
    for page in pages:
        builder.row(
            InlineKeyboardButton(
                text=f"üëç –°–ª–∞–π–¥ {page}",
                callback_data=f"fb_{query_hash}_{page}_1"
            ),
            InlineKeyboardButton(
                text=f"üëé –°–ª–∞–π–¥ {page}",
                callback_data=f"fb_{query_hash}_{page}_0"
            )
        )
    return builder.as_markup()


@dp.message(CommandStart())
async def start_command(message: types.Message):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã /start"""
    await message.answer(
        "üîç –†–∞–¥ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –≤–∞—Å –≤ —Å–∏—Å—Ç–µ–º–µ –¥–æ—Å—Ç—É–ø–∞ –∫ –ë–∞–∑–µ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –†–µ—à–µ–Ω–∏–π!!\n"
        "–û–ø–∏—à–∏—Ç–µ –∑–∞–¥–∞—á—É ‚Äî –∏ —è –ø–æ–¥–±–µ—Ä—É –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –∏–∑ –ë–¢–†.\n\n"
        "üîé –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤: \n"
        " ‚Ä¢ –ü–æ–∫—Ä—ã—Ç–∏–µ –≤ –¥–µ—Ç—Å–∫–æ–π\n"
        " ‚Ä¢ –£–ø–ª–æ—Ç–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∑–∏–Ω–∫–∏\n\n"
        
         "–ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º–∞ –∏ –≤ –ë–¢–† –Ω–µ—Ç —Ä–µ—à–µ–Ω–∏—è, –Ω–∞–ø–∏—à–∏—Ç–µ @VADCM,\n"
        "–õ–∏–±–æ –æ–ø–∏—à–∏—Ç–µ –ø—Ä–æ–±–ª–µ–º—É –≤ –Ω–∞—à–µ–π –≥—Ä—É–ø–ø–µ –î–æ–¥–æ –≠–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è –≤ –¢–ì –∏ –º—ã –ø–æ–º–æ–∂–µ–º: \n\n"
        "https://t.me/+Nu_BhZYYqoljNmZi"

    )

# —É–±—Ä–∞–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–º–æ—â–∏
#@dp.message(Command("help"))
#async def help_command(message: types.Message):
#    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã /help"""
#    help_text = (
#        "üõ† **–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:**\n\n"
#        "/start - –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –±–æ—Ç–∞\n"
#        "/help - –°–ø—Ä–∞–≤–∫–∞\n"
#        "/history - –ò—Å—Ç–æ—Ä–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤\n\n"
#        "üîé **–ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:**\n"
#        "‚Ä¢ –∑–∞–º–µ–Ω–∞ —Å—Ç–æ–ª–µ—à–Ω–∏—Ü \n"
#        "‚Ä¢ —Å—Ç—É–ª—å—è \n"
#        "‚Ä¢ —Ä–µ–∑–∏–Ω–∫–∞ –¥–ª—è —Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫–∞"
#    )
#    await message.answer(help_text, parse_mode=ParseMode.HTML)


# —Ñ—É–Ω–∫—Ü–∏—è –≤—ã–≤–æ–¥–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–∞–π–¥–∞
@dp.message(Command("stats999"))
async def send_feedback_stats(message: types.Message):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /stats999"""
    try:
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –Ω–∞—á–∞–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        processing_msg = await message.answer("‚è≥ –ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –∏ —Å—á–∏—Ç–∞—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É...")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        decoded_data = load_feedback_data()
        aggregated_data = aggregate_feedback_stats(decoded_data)
        stats_message = format_stats_message(aggregated_data)

        # –£–¥–∞–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –æ–±—Ä–∞–±–æ—Ç–∫–µ
        await bot.delete_message(
            chat_id=message.chat.id,
            message_id=processing_msg.message_id
        )

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        await message.answer(stats_message, parse_mode="HTML")

    except Exception as e:
        error_msg = f"‚ö†Ô∏è –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}"
        await message.answer(error_msg)
        # –î–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        print(f"Error in /stats999: {e}")


@dp.message(Command("history"))
async def history_command(message: types.Message):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã /history"""
    try:
        if not os.path.exists(Config.LOG_FILE_PATH):
            await message.answer("–ò—Å—Ç–æ—Ä–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –ø—É—Å—Ç–∞.")
            return

        async with aiofiles.open(Config.LOG_FILE_PATH, "r") as f:
            content = await f.read()

        user_entries = [
            line for line in content.splitlines()
            if str(message.from_user.id) in line
        ]

        if not user_entries:
            await message.answer("–í–∞—à–∞ –∏—Å—Ç–æ—Ä–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –ø—É—Å—Ç–∞.")
            return

        response = "üìú **–ò—Å—Ç–æ—Ä–∏—è –≤–∞—à–∏—Ö –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 5 –∑–∞–ø—Ä–æ—Å–æ–≤:**\n\n"
        for entry in user_entries[-5:]:
            parts = entry.split("; ")
            response += (
                f"üìÖ *{parts[0]}*\n"
                f"üîé *–ó–∞–ø—Ä–æ—Å:* {parts[4]}\n"
                f"üìë *–°–ª–∞–π–¥—ã:* {parts[5]}\n\n"
            )

        await message.answer(response, parse_mode=ParseMode.HTML)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏: {e}")
        await message.answer("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é.")



@dp.callback_query(lambda c: c.data.startswith('fb_'))
async def handle_feedback(callback: types.CallbackQuery):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏"""
    try:
        _, query_hash, page, feedback = callback.data.split('_')
        page_num = int(page)
        is_helpful = bool(int(feedback))
        user = callback.from_user

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å
        AIState.feedback[(query_hash, page_num)].append((user.id, is_helpful))
        await save_feedback()
        await adjust_search_weights()

        await callback.answer("–°–ø–∞—Å–∏–±–æ –∑–∞ –≤–∞—à –æ—Ç–∑—ã–≤! –≠—Ç–æ —É–ª—É—á—à–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞.")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏: {e}")
        await callback.answer("‚ö†Ô∏è –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –æ—Ç–∑—ã–≤–∞.")


async def adjust_search_weights():
    """–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –≤–µ—Å–æ–≤ –ø–æ–∏—Å–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏"""
    try:
        total_feedback = sum(len(fbs) for fbs in AIState.feedback.values())
        if total_feedback == 0:
            return

        positive = sum(1 for fbs in AIState.feedback.values() for fb in fbs if fb[1])
        positive_ratio = positive / total_feedback

        AIState.search_weights['feedback'] = min(0.5, positive_ratio)
        AIState.search_weights['keyword'] = 0.4 - (positive_ratio * 0.2)
        AIState.search_weights['semantic'] = 0.4 + (positive_ratio * 0.2)

        total = sum(AIState.search_weights.values())
        for k in AIState.search_weights:
            AIState.search_weights[k] /= total

        logging.info(f"–û–±–Ω–æ–≤–ª–µ–Ω—ã –≤–µ—Å–∞ –ø–æ–∏—Å–∫–∞: {AIState.search_weights}")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∫–∏ –≤–µ—Å–æ–≤: {e}")


def pdf_has_changed():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ PDF-—Ñ–∞–π–ª–µ"""
    try:
        current_hash = hashlib.md5(open(Config.PDF_FILE_PATH, "rb").read()).hexdigest()
        saved_hash = ""
        if os.path.exists("pdf_hash.txt"):
            with open("pdf_hash.txt", "r") as f:
                saved_hash = f.read()

        if current_hash != saved_hash:
            with open("pdf_hash.txt", "w") as f:
                f.write(current_hash)
            return True
        return False
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π PDF: {e}")
        return False


async def periodic_retraining():
    """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
    while True:
        await asyncio.sleep(Config.RETRAIN_INTERVAL)
        try:
            logging.info("–ù–∞—á–∞–ª–æ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è")

            if pdf_has_changed():
                logging.info("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ PDF")
                await precompute_embeddings()
                await build_inverse_index()

            await adjust_search_weights()
            logging.info("–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è: {e}")


@dp.message()
async def handle_query(message: types.Message):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
    try:
        user = message.from_user
        query = message.text.strip()
        if len(query) < 3:
            await message.answer("‚ùå –ó–∞–ø—Ä–æ—Å –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–∏–Ω–∏–º—É–º 3 —Å–∏–º–≤–æ–ª–∞.")
            return

        toxicity = await asyncio.to_thread(classifier, query)
        if toxicity[0]['label'] == 'toxic':
            await message.answer("üö´ –ó–∞–ø—Ä–æ—Å —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è.")
            return

        found_pages = await hybrid_search(query)
        await log_query(user, query, found_pages)

        if not found_pages:
            suggestions = get_close_matches(query, AIState.unique_words, n=5, cutoff=0.4)
            response = "üîç –ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."
            if suggestions:
                response += "\n–í–æ–∑–º–æ–∂–Ω–æ, –≤—ã –∏–º–µ–ª–∏ –≤ –≤–∏–¥—É:\n" + "\n".join(f"‚Ä¢ {s}" for s in suggestions)
            await message.answer(response, parse_mode=ParseMode.HTML)
            return

        context = "\n".join(AIState.text_cache[p] for p in found_pages)
        answer = await generate_answer(query, context)

        response = (
            f"üîç <b>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É:</b> <code>{escape_html(query)}</code>\n"
            f"üìñ <b>–°–ª–∞–π–¥—ã –∏–∑ –ë–¢–†: </b> {', '.join(map(str, found_pages))}"
        )

        await message.answer(response, parse_mode=ParseMode.HTML)

        for page_num in found_pages[:Config.MAX_PAGES_TO_SHOW]:
            img_path, links = await cache_page(page_num)

            if img_path and os.path.exists(img_path):
                await message.answer_photo(FSInputFile(img_path))

            formatted_links = []
            if links:
                for i, (link_text, link_url) in enumerate(links, 1):
                    if link_url and isinstance(link_url, str):
                        safe_url = escape_html(link_url.strip())
                        safe_text = escape_html(link_text.strip())
                        formatted_links.append(
                            f"{i}. <a href='{safe_url}'>{safe_text}</a>"
                        )

            if formatted_links:
                links_text = "\n".join(formatted_links)
                await message.answer(
                    f"üîó <b>–°—Å—ã–ª–∫–∏ –Ω–∞ —Å–ª–∞–π–¥–µ {page_num}:</b>\n{links_text}",
                    parse_mode=ParseMode.HTML,
                    disable_web_page_preview=True
                )

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—É—é –∫–ª–∞–≤–∏–∞—Ç—É—Ä—É –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            keyboard = create_feedback_keyboard(query, [page_num])
            await message.answer(
                f"üìå –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–∞ —Å–ª–∞–π–¥–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∑–∞–ø—Ä–æ—Å—É?\n\n",
                reply_markup=keyboard

            )

            # –í–∏–∑—É–∞–ª—å–Ω—ã–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –º–µ–∂–¥—É —Å–ª–∞–π–¥–∞–º–∏
            #await message.answer("‚éØ‚éØ‚éØ‚éØ")

    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}", exc_info=True)
        await message.answer("‚ö†Ô∏è –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞.")


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    await initialize_data()
    asyncio.create_task(periodic_retraining())
    await dp.start_polling(bot)


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logging.info("–ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
    except Exception as e:
        logging.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –±–æ—Ç–∞: {e}")